{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Diffusion Procedure"
      ],
      "metadata": {
        "id": "dJp3emqablIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqn9IIdebwVr",
        "outputId": "37a6421d-e39d-4b77-a661-d5a0cc2ce6aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DScWA_RFsPa9",
        "outputId": "b34dd122-4982-4d2f-dc61-475249c00e0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hfo7ECUMaOC9"
      },
      "outputs": [],
      "source": [
        "# Defining the diffusion procedure\n",
        "\n",
        "from typing import Tuple, Optional\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "\n",
        "import torch.utils.data\n",
        "\n",
        "\n",
        "def gather(consts: torch.Tensor, t: torch.Tensor):\n",
        "    \"\"\"Gather consts for $t$ and reshape to feature map shape\"\"\"\n",
        "    c = consts.gather(-1, t)\n",
        "    return c.reshape(-1, 1, 1, 1)\n",
        "\n",
        "\n",
        "class DenoiseDiffusion:\n",
        "    def __init__(self, latent_model: nn.Module, eps_model: nn.Module, n_steps: int, device: torch.device):\n",
        "        # super().__init__()\n",
        "        self.device = device\n",
        "        self.latent_model = latent_model\n",
        "        self.eps_model = eps_model\n",
        "        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(device)\n",
        "        self.alpha = 1. - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.n_steps = n_steps\n",
        "        self.sigma2 = self.beta\n",
        "\n",
        "    def q_xt_x0(self, x0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        ''' Note : x0 is in the latent space'''\n",
        "        mean = gather(self.alpha_bar, t) ** 0.5 * x0\n",
        "        var = 1 - gather(self.alpha_bar, t)\n",
        "        return mean, var\n",
        "\n",
        "    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor] = None):\n",
        "        ''' Note : x0 is in the latent space'''\n",
        "        if eps is None:\n",
        "            eps = torch.randn_like(x0)\n",
        "\n",
        "        mean, var = self.q_xt_x0(x0, t)\n",
        "        return mean + (var ** 0.5) * eps\n",
        "\n",
        "    def p_sample(self, xt: torch.Tensor, t: torch.Tensor, map_latent=False):\n",
        "        if map_latent:\n",
        "            pass # removed the latent Encoder\n",
        "        eps_theta = self.eps_model(xt, t)\n",
        "        alpha_bar = gather(self.alpha_bar, t)\n",
        "        alpha = gather(self.alpha, t)\n",
        "        eps_coef = (1 - alpha) / (1 - alpha_bar) ** .5\n",
        "        mean = 1 / (alpha ** 0.5) * (xt - eps_coef * eps_theta)\n",
        "        var = gather(self.sigma2, t)\n",
        "        eps = torch.randn(xt.shape, device=xt.device)\n",
        "        return mean + (var ** .5) * eps\n",
        "    \n",
        "    def reconstruct(self, xt: torch.Tensor, eps_theta: torch.Tensor, t: torch.Tensor):\n",
        "        out = self.p_x0(xt, t, eps_theta)\n",
        "        return out\n",
        "\n",
        "    def loss(self, x0: torch.Tensor, noise: Optional[torch.Tensor] = None, reconstruct: bool = False):\n",
        "        recon = None\n",
        "        batch_size = x0.shape[0]\n",
        "        x0 = x0.to(self.device)\n",
        "        t = torch.randint(0, self.n_steps, (batch_size,), device=x0.device, dtype=torch.long)\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "        xt = self.q_sample(x0, t, eps=noise)\n",
        "        eps_theta = self.eps_model(xt, t)\n",
        "        if reconstruct:\n",
        "            recon = self.reconstruct(xt, eps_theta, t)\n",
        "        self.loss_ = F.mse_loss(noise, eps_theta)\n",
        "        return self.loss_, recon\n",
        "    \n",
        "    def serialize_eps_model(self, iter_number, optimizer, loss):\n",
        "        path = '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_'+str(iter_number)\n",
        "        torch.save({\n",
        "            'iteration_number': iter_number,\n",
        "            'model_state_dict': self.eps_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, path)\n",
        "    \n",
        "    def p_x0(self, xt: torch.Tensor, t: torch.Tensor, eps: torch.Tensor):\n",
        "\n",
        "        alpha_bar = gather(self.alpha_bar, t)\n",
        "        return (xt - (1 - alpha_bar) ** 0.5 * eps) / (alpha_bar ** 0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "YJHiph07bhDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets for training\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from glob import glob\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def by_sample_number(path):\n",
        "    num = int(path.split(os.path.sep)[-1].split('.')[0])\n",
        "    return num\n",
        "\n",
        "\n",
        "class MNISTDataset(torchvision.datasets.MNIST):\n",
        "    \"\"\"\n",
        "    ### MNIST dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size, data_path):\n",
        "        transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize(image_size),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "        ])\n",
        "        self.data_path = data_path\n",
        "        super().__init__(self.data_path, train=True, download=True, transform=transform)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return super().__getitem__(item)[0]\n",
        "\n",
        "class CarsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset_path):\n",
        "        self.imsize = 224\n",
        "        self.dataset_path = dataset_path\n",
        "        self._load_dataset_paths()\n",
        "    \n",
        "    def _load_dataset_paths(self):\n",
        "        self.all_samples = sorted(glob(self.dataset_path + os.path.sep + '*.jpg'), key=by_sample_number)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = cv2.imread(self.all_samples[index])\n",
        "        img = cv2.resize(img, (self.imsize, self.imsize))\n",
        "        img = img.transpose(2, 0, 1)\n",
        "        assert img.shape == (3, self.imsize, self.imsize)\n",
        "        assert np.max(img) <= 255\n",
        "        img = torch.FloatTensor(img / 255.)\n",
        "        return img\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.all_samples)\n"
      ],
      "metadata": {
        "id": "0F_jTCeLaZgW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Model Implementation"
      ],
      "metadata": {
        "id": "-jv9KohQbbll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the UNet Model\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "UNET implementation containing a bunch of modifications (residual blocks, multi-head attention, time-step embedding t)\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from typing import Optional, Tuple, Union, List\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Module\n",
        "\n",
        "\n",
        "class Swish(Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, n_channels: int):\n",
        "        super().__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
        "        self.act = Swish()\n",
        "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
        "\n",
        "    def forward(self, t: torch.Tensor):\n",
        "        half_dim = self.n_channels // 8\n",
        "        emb = math.log(10_000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
        "        emb = t[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
        "        emb = self.act(self.lin1(emb))\n",
        "        emb = self.lin2(emb)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class ResidualBlock(Module):\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, n_groups: int = 32):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
        "        self.act1 = Swish()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
        "        self.act2 = Swish()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        h = self.conv1(self.act1(self.norm1(x)))\n",
        "        h += self.time_emb(t)[:, :, None, None]\n",
        "        h = self.conv2(self.act2(self.norm2(h)))\n",
        "        return h + self.shortcut(x)\n",
        "\n",
        "\n",
        "class AttentionBlock(Module):\n",
        "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if d_k is None:\n",
        "            d_k = n_channels\n",
        "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
        "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
        "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
        "        self.scale = d_k ** -0.5\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
        "        _ = t\n",
        "        batch_size, n_channels, height, width = x.shape\n",
        "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
        "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
        "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
        "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
        "        attn = attn.softmax(dim=2)\n",
        "        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n",
        "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
        "        res = self.output(res)\n",
        "        res += x\n",
        "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
        "        return res\n",
        "\n",
        "\n",
        "class DownBlock(Module):\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
        "        super().__init__()\n",
        "        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        x = self.res(x, t)\n",
        "        x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpBlock(Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
        "        super().__init__()\n",
        "        self.res = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        x = self.res(x, t)\n",
        "        x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MiddleBlock(Module):\n",
        "    def __init__(self, n_channels: int, time_channels: int):\n",
        "        super().__init__()\n",
        "        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n",
        "        self.attn = AttentionBlock(n_channels)\n",
        "        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        x = self.res1(x, t)\n",
        "        x = self.attn(x)\n",
        "        x = self.res2(x, t)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, n_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        _ = t\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "\n",
        "    def __init__(self, n_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        _ = t\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(Module):\n",
        "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
        "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
        "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
        "                 n_blocks: int = 2):\n",
        "        super().__init__()\n",
        "        n_resolutions = len(ch_mults)\n",
        "\n",
        "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "\n",
        "        self.time_emb = TimeEmbedding(n_channels * 4)\n",
        "\n",
        "        down = []\n",
        "        out_channels = in_channels = n_channels\n",
        "        for i in range(n_resolutions):\n",
        "            out_channels = in_channels * ch_mults[i]\n",
        "            for _ in range(n_blocks):\n",
        "                down.append(DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
        "                in_channels = out_channels\n",
        "            if i < n_resolutions - 1:\n",
        "                down.append(Downsample(in_channels))\n",
        "\n",
        "        self.down = nn.ModuleList(down)\n",
        "        self.middle = MiddleBlock(out_channels, n_channels * 4, )\n",
        "        up = []\n",
        "        in_channels = out_channels\n",
        "        for i in reversed(range(n_resolutions)):\n",
        "            out_channels = in_channels\n",
        "            for _ in range(n_blocks):\n",
        "                up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
        "            out_channels = in_channels // ch_mults[i]\n",
        "            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
        "            in_channels = out_channels\n",
        "            if i > 0:\n",
        "                up.append(Upsample(in_channels))\n",
        "\n",
        "        self.up = nn.ModuleList(up)\n",
        "        self.norm = nn.GroupNorm(8, n_channels)\n",
        "        self.act = Swish()\n",
        "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        t = self.time_emb(t)\n",
        "        x = self.image_proj(x)\n",
        "        h = [x]\n",
        "        for m in self.down:\n",
        "            x = m(x, t)\n",
        "            h.append(x)\n",
        "\n",
        "        x = self.middle(x, t)\n",
        "\n",
        "        for m in self.up:\n",
        "            if isinstance(m, Upsample):\n",
        "                x = m(x, t)\n",
        "            else:\n",
        "                s = h.pop()\n",
        "                x = torch.cat((x, s), dim=1)\n",
        "                x = m(x, t)\n",
        "        return self.final(self.act(self.norm(x)))\n"
      ],
      "metadata": {
        "id": "SIKw6mq8agM0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traning and Sampling"
      ],
      "metadata": {
        "id": "SAHadGn-bWjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and sampling\n",
        "\n",
        "from typing import List\n",
        "import torch\n",
        "import torch.utils.data\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  !pip3 install torchinfo\n",
        "  from torchinfo import summary\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from tqdm import trange\n",
        "import cv2\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.ion()\n",
        "\n",
        "def by_iter_num(path):\n",
        "    num = int(path.split(os.path.sep)[-1].split('_')[-1])\n",
        "    return num\n",
        "\n",
        "\n",
        "class Configs:\n",
        "    device: torch.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    iter_counter : int = 0\n",
        "\n",
        "    eps_model: UNet\n",
        "    diffusion: DenoiseDiffusion\n",
        "    image_channels: int = 3\n",
        "    image_size: int = 224\n",
        "    n_channels: int = 64\n",
        "    channel_multipliers: List[int] = [1, 2, 4]\n",
        "    is_attention: List[int] = [False, False, True]\n",
        "\n",
        "    n_steps: int = 1_000 # number of diffusion steps\n",
        "    batch_size: int = 4\n",
        "    n_samples: int = 4\n",
        "    learning_rate: float = 2e-5\n",
        "    epochs: int = 1_000\n",
        "    dataset_path: str = '/content/drive/MyDrive/diffusion_dataset/train/' \n",
        "    dataset: torch.utils.data.Dataset = CarsDataset(dataset_path)\n",
        "    data_loader: torch.utils.data.DataLoader\n",
        "    optimizer: torch.optim.Adam\n",
        "\n",
        "    def init(self, debug=True, cold_start=True):\n",
        "        if cold_start:\n",
        "            self.eps_model = UNet(\n",
        "                image_channels=self.image_channels,\n",
        "                n_channels=self.n_channels,\n",
        "                ch_mults=self.channel_multipliers,\n",
        "                is_attn=self.is_attention,\n",
        "            ).to(self.device)\n",
        "            self.optimizer = torch.optim.Adam(self.eps_model.parameters(), lr=self.learning_rate)\n",
        "        else:\n",
        "            self.eps_model = UNet(\n",
        "                image_channels=self.image_channels,\n",
        "                n_channels=self.n_channels,\n",
        "                ch_mults=self.channel_multipliers,\n",
        "                is_attn=self.is_attention,\n",
        "            )\n",
        "            print(sorted(glob('/content/drive/MyDrive/diffusion_dataset/model_checkpoints/*'), key=by_iter_num))\n",
        "            chkpt_path = sorted(glob('/content/drive/MyDrive/diffusion_dataset/model_checkpoints/*'), key=by_iter_num)[-1]\n",
        "            state_dict = torch.load(chkpt_path)\n",
        "            self.eps_model.load_state_dict(state_dict['model_state_dict'])\n",
        "            self.eps_model = self.eps_model.to(self.device)\n",
        "            self.optimizer = torch.optim.Adam(self.eps_model.parameters(), lr=self.learning_rate)\n",
        "            self.optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "            self.iter_counter = int(state_dict['iteration_number'])\n",
        "        if debug:\n",
        "            summary(self.eps_model, input_data=(torch.from_numpy(np.random.uniform(size=(1, 3, 224, 224))).type(torch.float32).to(self.device),\n",
        "                                                 torch.randint(0, self.n_steps, (1,), device=self.device, dtype=torch.long)) )\n",
        "            os.system('nvidia-smi')\n",
        "        self.diffusion = DenoiseDiffusion(\n",
        "            latent_model=None,\n",
        "            eps_model=self.eps_model,\n",
        "            n_steps=self.n_steps,\n",
        "            device=self.device,\n",
        "        )\n",
        "        self.data_loader = torch.utils.data.DataLoader(self.dataset, self.batch_size, shuffle=True, pin_memory=True)\n",
        "        \n",
        "\n",
        "    def show_output(self, output):\n",
        "        show_image = np.moveaxis(output.detach().cpu().numpy()[0], 0, -1)\n",
        "        show_image = np.clip(show_image, 0, 1)\n",
        "        plt.cla()\n",
        "        plt.imshow(show_image)\n",
        "        plt.draw()\n",
        "        plt.pause(0.001)\n",
        "        return\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"\n",
        "        ### Sample images\n",
        "        \"\"\"\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            x = torch.randn([1, self.image_channels, self.image_size, self.image_size],\n",
        "                            device=self.device)\n",
        "\n",
        "            for t_ in trange(self.n_steps):\n",
        "                \n",
        "                t = self.n_steps - t_ - 1\n",
        "                x = self.diffusion.p_sample(x, x.new_full((1,), t, dtype=torch.long))\n",
        "\n",
        "                if t_ % 50 == 0:\n",
        "                    self._serialize_generations(x.detach().cpu(), prefix=str(t_))\n",
        "            for _ in range(15):\n",
        "                t = 0\n",
        "                x = self.diffusion.p_sample(x, x.new_full((1,), t, dtype=torch.long))\n",
        "\n",
        "                if t % 50 == 0:\n",
        "                    self._serialize_generations(x.detach().cpu(), prefix=str(t_))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, reconstruct=False):\n",
        "        for _, data in enumerate(tqdm(self.data_loader)):\n",
        "            self.iter_counter += 1\n",
        "            data = data.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss, recon = self.diffusion.loss(data, reconstruct=reconstruct)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            if reconstruct:\n",
        "                self.show_output(recon)\n",
        "            # print('[INFO] Current Batch Loss : ', loss.item())\n",
        "            # print('Iter counter : ', self.iter_counter)\n",
        "            if self.iter_counter % 2_000 == 0: # saves the model every 1500 updates\n",
        "                print('[INFO] Serializing model...')\n",
        "                self.diffusion.serialize_eps_model(self.iter_counter, self.optimizer, loss)\n",
        "\n",
        "    def _serialize_generations(self, generations, prefix=None):\n",
        "        generations = generations.numpy()\n",
        "\n",
        "        if not os.path.exists('./gen_images'):\n",
        "          os.mkdir('./gen_images')\n",
        "\n",
        "        for k, gen_img in enumerate(generations):\n",
        "            show_image = np.moveaxis(gen_img, 0, -1)\n",
        "            show_image = np.clip(show_image, 0, 1)\n",
        "            show_image = (show_image * 255).astype(np.uint8)\n",
        "            if not prefix is None:\n",
        "                cv2.imwrite(f'./gen_images/{prefix}{k}.png', show_image)\n",
        "            else:\n",
        "                cv2.imwrite(f'./gen_images/{k}.png', show_image)\n",
        "        \n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        ### Training loop\n",
        "        \"\"\"\n",
        "        for _ in range(self.epochs):\n",
        "            self.train(reconstruct=False)\n",
        "    \n",
        "    def generate_samples(self):\n",
        "      \"\"\"\n",
        "      ### Generate Samples from Noise\n",
        "      \"\"\"\n",
        "      self.sample()\n",
        "\n",
        "\n",
        "def main():\n",
        "    configs = Configs()\n",
        "    configs.init(cold_start=False) # setting cold_start to False loads the most recent checkpoint\n",
        "    # configs.run() # trains the diffusion model\n",
        "    configs.generate_samples() # for running inference\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH02J35-aqwG",
        "outputId": "1d7066d1-0edc-4b81-8007-341df68b22e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_102000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_103500', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_103600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_103800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_104000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_104200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_104400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_104600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_104800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_105000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_105200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_105400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_105600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_105800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_106000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_106200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_106400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_106600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_106800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_107000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_107200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_107400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_107600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_107800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_108000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_108200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_108400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_108600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_108800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_109000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_109200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_109400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_109600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_109800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_110000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_110200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_110400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_110600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_110800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_111000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_111200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_111400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_111600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_111800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_112000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_112200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_112400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_112600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_112800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_113000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_113200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_113400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_113600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_113800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_114000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_114200', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_114400', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_114600', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_114800', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_115000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_116000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_118000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_120000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_122000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_124000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_126000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_128000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_130000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_132000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_134000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_136000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_138000', '/content/drive/MyDrive/diffusion_dataset/model_checkpoints/model_checkpoint_140000']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:55<00:00,  8.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/diffusion_dataset/model_checkpoints"
      ],
      "metadata": {
        "id": "osqzGiPXshdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JqplcvENvRyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "!zip -r ./gen_images1.zip ./gen_images\n",
        "!files.download('./gen_images1.zip')"
      ],
      "metadata": {
        "id": "Esly4CycxNGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b95b21-c5fa-4c3f-d18b-972a43a3b2e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: gen_images/ (stored 0%)\n",
            "  adding: gen_images/9501.png (deflated 0%)\n",
            "  adding: gen_images/8000.png (deflated 0%)\n",
            "  adding: gen_images/2502.png (deflated 0%)\n",
            "  adding: gen_images/1001.png (deflated 0%)\n",
            "  adding: gen_images/501.png (deflated 0%)\n",
            "  adding: gen_images/8003.png (deflated 0%)\n",
            "  adding: gen_images/4500.png (deflated 0%)\n",
            "  adding: gen_images/7000.png (deflated 0%)\n",
            "  adding: gen_images/9990.png (deflated 0%)\n",
            "  adding: gen_images/3501.png (deflated 0%)\n",
            "  adding: gen_images/8501.png (deflated 0%)\n",
            "  adding: gen_images/4503.png (deflated 0%)\n",
            "  adding: gen_images/4001.png (deflated 0%)\n",
            "  adding: gen_images/3001.png (deflated 0%)\n",
            "  adding: gen_images/5002.png (deflated 0%)\n",
            "  adding: gen_images/3503.png (deflated 0%)\n",
            "  adding: gen_images/3003.png (deflated 0%)\n",
            "  adding: gen_images/4501.png (deflated 0%)\n",
            "  adding: gen_images/6002.png (deflated 0%)\n",
            "  adding: gen_images/8001.png (deflated 0%)\n",
            "  adding: gen_images/5001.png (deflated 0%)\n",
            "  adding: gen_images/7001.png (deflated 0%)\n",
            "  adding: gen_images/6001.png (deflated 0%)\n",
            "  adding: gen_images/2500.png (deflated 0%)\n",
            "  adding: gen_images/7500.png (deflated 0%)\n",
            "  adding: gen_images/1003.png (deflated 0%)\n",
            "  adding: gen_images/4003.png (deflated 0%)\n",
            "  adding: gen_images/4002.png (deflated 0%)\n",
            "  adding: gen_images/2003.png (deflated 0%)\n",
            "  adding: gen_images/03.png (deflated 0%)\n",
            "  adding: gen_images/6000.png (deflated 0%)\n",
            "  adding: gen_images/7502.png (deflated 0%)\n",
            "  adding: gen_images/8503.png (deflated 0%)\n",
            "  adding: gen_images/5003.png (deflated 0%)\n",
            "  adding: gen_images/1002.png (deflated 0%)\n",
            "  adding: gen_images/5502.png (deflated 0%)\n",
            "  adding: gen_images/1502.png (deflated 0%)\n",
            "  adding: gen_images/00.png (deflated 0%)\n",
            "  adding: gen_images/6503.png (deflated 0%)\n",
            "  adding: gen_images/7501.png (deflated 0%)\n",
            "  adding: gen_images/1000.png (deflated 0%)\n",
            "  adding: gen_images/9503.png (deflated 0%)\n",
            "  adding: gen_images/7003.png (deflated 0%)\n",
            "  adding: gen_images/3500.png (deflated 0%)\n",
            "  adding: gen_images/2002.png (deflated 0%)\n",
            "  adding: gen_images/1500.png (deflated 0%)\n",
            "  adding: gen_images/9001.png (deflated 0%)\n",
            "  adding: gen_images/02.png (deflated 0%)\n",
            "  adding: gen_images/6500.png (deflated 0%)\n",
            "  adding: gen_images/1501.png (deflated 0%)\n",
            "  adding: gen_images/8002.png (deflated 0%)\n",
            "  adding: gen_images/1503.png (deflated 0%)\n",
            "  adding: gen_images/500.png (deflated 0%)\n",
            "  adding: gen_images/5500.png (deflated 0%)\n",
            "  adding: gen_images/5503.png (deflated 0%)\n",
            "  adding: gen_images/9003.png (deflated 0%)\n",
            "  adding: gen_images/4000.png (deflated 0%)\n",
            "  adding: gen_images/5000.png (deflated 0%)\n",
            "  adding: gen_images/6003.png (deflated 0%)\n",
            "  adding: gen_images/4502.png (deflated 0%)\n",
            "  adding: gen_images/6502.png (deflated 0%)\n",
            "  adding: gen_images/9000.png (deflated 0%)\n",
            "  adding: gen_images/503.png (deflated 0%)\n",
            "  adding: gen_images/2000.png (deflated 0%)\n",
            "  adding: gen_images/3000.png (deflated 0%)\n",
            "  adding: gen_images/8502.png (deflated 0%)\n",
            "  adding: gen_images/9993.png (deflated 0%)\n",
            "  adding: gen_images/7503.png (deflated 0%)\n",
            "  adding: gen_images/3502.png (deflated 0%)\n",
            "  adding: gen_images/7002.png (deflated 0%)\n",
            "  adding: gen_images/9002.png (deflated 0%)\n",
            "  adding: gen_images/9992.png (deflated 0%)\n",
            "  adding: gen_images/01.png (deflated 0%)\n",
            "  adding: gen_images/9502.png (deflated 0%)\n",
            "  adding: gen_images/6501.png (deflated 0%)\n",
            "  adding: gen_images/8500.png (deflated 0%)\n",
            "  adding: gen_images/3002.png (deflated 0%)\n",
            "  adding: gen_images/2503.png (deflated 0%)\n",
            "  adding: gen_images/5501.png (deflated 0%)\n",
            "  adding: gen_images/2501.png (deflated 0%)\n",
            "  adding: gen_images/502.png (deflated 0%)\n",
            "  adding: gen_images/9500.png (deflated 0%)\n",
            "  adding: gen_images/9991.png (deflated 1%)\n",
            "  adding: gen_images/2001.png (deflated 0%)\n",
            "/bin/bash: -c: line 0: syntax error near unexpected token `'./gen_images1.zip''\n",
            "/bin/bash: -c: line 0: `files.download('./gen_images1.zip')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hqDyHn3PeQrX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}